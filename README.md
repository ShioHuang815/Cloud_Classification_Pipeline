# Cloud Classification Pipeline

A reproducible pipeline for classifying cloud types based on image features, with automated data cleaning, model training, and artifact storage.

![Cloud Classification](https://img.shields.io/badge/Machine%20Learning-Cloud%20Classification-blue)
![Python](https://img.shields.io/badge/Python-3.9%2B-brightgreen)
![Docker](https://img.shields.io/badge/Docker-Compatible-blue)
![License](https://img.shields.io/badge/License-MIT-green)

## ğŸ“‹ Overview

This project implements an end-to-end pipeline for cloud classification using machine learning. The system processes cloud imagery, extracts relevant features, trains classification models, and stores resulting artifacts for deployment or further analysis.

## ğŸ”§ Prerequisites

- Docker
- Python 3.9+
- AWS CLI (optional, for S3 access)

## ğŸš€ Setup

1. Clone the repository:
   ```bash
   git clone https://github.com/your-repo/cloud-classification.git
   cd cloud-classification
   ```

2. Build the Docker image:
   ```bash
   docker build -t cloud-classifier -f dockerfiles/Dockerfile .
   ```

## ğŸƒâ€â™‚ï¸ Running the Pipeline

### Local Execution
```bash
python pipeline.py
```

### Docker Execution
```bash
docker run -v $(pwd)/data:/app/data \
           -v $(pwd)/artifacts:/app/artifacts \
           cloud-classifier
```

## ğŸ§ª Testing

### Running Tests Locally
```bash
python tests/unit_tests.py
```

### Running Tests in Docker
```bash
docker run cloud-classifier python tests/unit_tests.py
```

## ğŸ“ Linting (PEP8 Compliance)

```bash
# Install pylint if needed
pip install pylint

# Run linting
docker run cloud-classifier pylint pipeline.py tests modules
```

## â˜ï¸ AWS Configuration (Optional)

To enable S3 uploads for artifact storage:

1. Set environment variables:
   ```bash
   export AWS_ACCESS_KEY_ID=your_access_key
   export AWS_SECRET_ACCESS_KEY=your_secret_key
   export AWS_BUCKET_NAME=your-bucket-name
   ```

2. Or configure in `config.yaml`:
   ```yaml
   aws:
     bucket_name: "your-bucket-name"
     s3_folder: "cloud-classifier"
     region_name: "us-east-1"
   ```

## ğŸ“ Folder Structure

```
.
â”œâ”€â”€ pipeline.py           # Main pipeline execution script
â”œâ”€â”€ README.md             # This documentation
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ .pylintrc             # Linting configuration
â”œâ”€â”€ data
â”‚   â””â”€â”€ clouds.csv        # Input dataset
â”œâ”€â”€ modules
â”‚   â”œâ”€â”€ data_cleaning.py  # Data preprocessing module
â”‚   â”œâ”€â”€ model_training.py # ML training module
â”‚   â””â”€â”€ aws_util.py       # AWS integration utilities
â”œâ”€â”€ config
â”‚   â””â”€â”€ config.yaml       # Configuration settings
â”œâ”€â”€ dockerfiles
â”‚   â””â”€â”€ Dockerfile        # Docker container definition
â””â”€â”€ tests
    â”œâ”€â”€ test_cleaning.py  # Unit tests for data cleaning
    â”œâ”€â”€ test_training.py  # Unit tests for model training
    â””â”€â”€ test_aws.py       # Unit tests for AWS utilities
```

## ğŸ“Š Pipeline Workflow

1. **Data Loading**: Imports cloud imagery data from CSV files
2. **Data Cleaning**: Preprocesses images, handles missing values
3. **Feature Extraction**: Identifies key cloud characteristics 
4. **Model Training**: Trains classification models using processed data
5. **Model Evaluation**: Evaluates performance metrics
6. **Artifact Storage**: Saves models and metadata locally or to S3

## ğŸ“ Notes

* All Python code complies with PEP8 standards
* The included `.pylintrc` ensures consistent linting
* Tests can be run both locally and in Docker containers
* AWS uploads are optional and require proper credentials

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Commit changes: `git commit -m 'Add feature'`
4. Push to branch: `git push origin feature-name`
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.
